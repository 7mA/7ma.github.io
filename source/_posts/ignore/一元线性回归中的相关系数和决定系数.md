---
title: 一元线性回归中的相关系数和决定系数
permalink: Correlation_Determination_in_Simple_Linear_Regression/
date: 2017-09-12 09:07:26
tags:
- 回归
- 统计学
categories:
- 技术笔记
---

变量之间存在的不确定的数量关系称为**相关关系（correlation）**。在一元线性回归分析中，能够一定程度描述出相关关系的量有两个：**相关系数（correlation coefficient）**和**决定系数（coefficient of determination，又译判定系数）**。相关系数是直接描述自变量和因变量之间数量关系的量，而决定系数代表的是线性回归方程得出的因变量变化量能被自变量解释的比例，即回归方程的拟合程度。两者虽然计算方式和概念来源不同，但是在描述一元线性回归中的自变量和因变量之间的关系上有一定相似的地方。

![](http://on5we0xu0.bkt.clouddn.com/17-9-12/36199622.jpg)

<!--more-->

---

## 1.散点图与相关系数

本文涉及到的是一元线性回归，因此只考虑两个变量之间的关系。要想找出两个变量之间存在的不确定的数量关系，相关分析是必不可少的步骤。我们希望在相关分析中得到以下问题的答案：

1. 变量之间是否存在关系？
2. 如果存在关系，是什么关系？
3. 这个关系的强度如何？
4. 样本中的变量关系是否能够代表总体变量之间的关系？

首先第一个问题，我们需要一个直观的工具来观察变量是否存在关系，这个工具就是**散点图（scatter diagram）**。通过散点图，我们不仅能看出两个变量之间是否有关系，还能大致看出关系类型与强度。下图是分别体现正线性相关和负线性相关关系的散点图：

![](http://on5we0xu0.bkt.clouddn.com/17-9-12/44544942.jpg)

如上图所示，如果两个变量的关系能近似表示成一条直线，则称为线性相关。特别地，如果样本点完全在一条直线上，我们就叫做完全线性相关。如果因变量随自变量增大而增大，则称为正相关，反之为负相关。

通过散点图我们可以大致确定关系是否存在及其类型。这也就解决了前两个问题。但是散点图无法准确告诉我们变量之间的关系强度。因此，我们需要相关系数来表示这个关系强度。相关系数分为总体相关系数和样本相关系数。前者是用总体全部数据计算的，用ρ表示，后者是用样本数据计算的，用r表示。在线性相关中，样本相关系数的计算方式如下：

> ![](http://on5we0xu0.bkt.clouddn.com/17-9-12/53709447.jpg)

用这个公式计算出来的相关系数也称为**线性相关系数（linear correlation coefficient）**或者**Pearson相关系数（Pearson's correlation coefficient）**。

相关系数拥有如下性质：

1. r∈[-1, +1]，如果r>0则说明两个变量呈正线性相关关系，r<0则说明是负线性相关关系。如果r=±1则说明两个变量呈完全线性相关关系。r=0说明两者不存在线性相关关系*（注意只是不存在线性相关关系，可能还存在其他相关关系）*。
2. r具有对称性。即r<sub>xy</sub>=r<sub>yx</sub>。r在数值和概念上不会体现出哪个变量是自变量或者因变量，需要与具体情境结合起来。
3. r的数值大小不与x和y的绝对大小有关，只与它们的大小关系有关。比如把x和y分别乘上100，r不会发生变化。

相关系数解决了第三个问题。第四个问题的解决需要用到相关关系的显著性检验，但是已经超出本文范围，所以这里不作详细解释。

---

## 2.决定系数

除了相关系数以外，还有一个量可以反映出两个变量之间的线性相关程度，即决定系数。决定系数是对估计的回归方程的拟合优度的度量。如果两个变量的线性关系越强，那么我们对应拟合出来的线性方程的拟合优度应该就越好。我们想要找拟合优度的目的就是*想要找出因变量和自变量的关系到底有多少比例是来自于拟合出来的方程的*。为了找出这个比例，我们需要从因变量y的变差入手。

一般来说，数据集中的y的取值是不尽相同的，这种y取值的波动叫做变差。变差的产生有可能来自于两个方面，一是自变量x的取值不同导致对应的y也不同，二是除与x的线性关系以外的因素（如x与y的非线性关系或误差等）影响造成的。对一个具体的观测值来说，变差大小可以用实际观测值和其均值之差表示：

> ![](http://on5we0xu0.bkt.clouddn.com/17-9-18/24484473.jpg)

所有观测值的总变差可以用这些变差的平方和表示，称为**总平方和（total sum of squares）**，记作SST：

> ![](http://on5we0xu0.bkt.clouddn.com/17-9-18/52880834.jpg)

观察每个观测点的变差的计算公式，可以分解为：

> ![](http://on5we0xu0.bkt.clouddn.com/17-9-18/23173449.jpg)

上式等号两边同时平方，并对所有观测点进行求和：

> ![](http://on5we0xu0.bkt.clouddn.com/17-9-18/21652934.jpg)

在最小二乘法求线性回归方程过程中，我们曾经令误差值Q关于回归系数β<sub>0</sub>的偏导数等于0使Q极小化：

> ![](http://on5we0xu0.bkt.clouddn.com/17-9-18/83555097.jpg)

因此可得：

> ![](http://on5we0xu0.bkt.clouddn.com/17-9-18/98454092.jpg)

因此有：

> ![](http://on5we0xu0.bkt.clouddn.com/17-9-18/10571447.jpg)

其中等号左侧称为总平方和SST，等号右边可以分为两部分。第一项是各实际观测点与回归值的残差的平方和，这一部分体现了除了x对y的线性影响以外其他因素对y变差的作用，是*不能用回归直线解释的变差部分*，称为**残差平方和**或**误差平方和（sum of squares of error）**，记作SSE；第二项是回归值与均值的差的平方和，可以看作是自变量x的变化导致的y的变化部分，是*可以用回归直线来解释的变差部分*，称为**回归平方和（sum of squares of regression)**，记作SSR ~~superior super rare~~。三者的关系为：

> SST=SSR+SSE

可以直观地看出，回归直线拟合程度的好坏取决于SSR与SSE的大小关系以及与SST的比例。观测点越接近直线，SSR/SST的值越大，直线拟合就越好。因此这里给出定义：回归平方和占总平方和的比例称为决定系数，记作R<sup>2</sup>。

R<sup>2</sup>的计算公式如下：

> ![](http://on5we0xu0.bkt.clouddn.com/17-9-18/82238671.jpg)

决定系数R<sup>2</sup>测度了回归直线对观测数据的拟合程度。如果所有观测点都在直线上，那么残差平方和SSE=0，R<sup>2</sup>=1，拟合是完全准确的；如果y的变化与x无关，完全无法解释y的变差，此时SSR=0，则R<sup>2</sup>=0。可见R<sup>2</sup>∈[0, 1]。R<sup>2</sup>越接近1，则说明回归平方和占总平方和的比例越大，回归直线与各观测点越接近，用x的变化来解释y的变差的部分就越多，回归直线拟合程度就越好；R<sup>2</sup>越接近0，回归直线的拟合程度就越差。

---

## 3.一元线性回归中相关系数和决定系数之间的关系

一元线性回归中，相关系数和决定系数之间在数值上有一定的关系。根据最小二乘法的解我们可以得到如下性质：

> ![](http://on5we0xu0.bkt.clouddn.com/17-9-19/50361456.jpg)

因此在一元线性回归下有如下式子：

> ![](http://on5we0xu0.bkt.clouddn.com/17-9-19/10188993.jpg)

所以有：

> ![](http://on5we0xu0.bkt.clouddn.com/17-9-19/32011545.jpg)

可以看到括号中的部分正是相关系数r。可见在一元线性回归当中，相关系数r实际上就是决定系数的平方根。这一结论不仅可以让我们直接用相关系数计算决定系数，还进一步解释了相关系数的意义。相关系数r的符号实际上和x项的回归系数正负号是一样的。相关系数也揭示出了回归直线的拟合优度。|r|越接近1，回归直线拟合度越高。
