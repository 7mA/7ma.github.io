---
title: 非线性回归复习笔记
permalink: Logistic_Regression_note/
date: 2017-09-01 22:48:43
tags:
- 机器学习
- 回归
categories:
- 技术笔记
---

最近在看麦子学院上的[深度学习基础介绍-机器学习](http://www.maiziedu.com/course/373/ "")的非线性回归理论部分时，发现自己三个月前看过的非线性回归的原理和推导有些混乱。于是就特地翻出了台湾大学林轩田教授的[《机器学习基石》课程](http://www.bilibili.com/video/av4294020/ "")复习了这一部分。为了巩固这一部分，特地做点笔记，以备将来复习。

![](http://on5we0xu0.bkt.clouddn.com/17-9-1/70406256.jpg)

<!--more-->

---

## 1.逻辑斯蒂回归问题

机器学习问题一般分为两个问题：**分类（Classification）**和**回归（Regression）**。分类问题通常应用在结果是离散的情况，根据结果个数可以分为二元分类、多元分类等等。比如说我们想知道明天下不下雨，我们就需要一系列的信息作为输入值特征向量，之后二元分类算法的目标函数可以给出我们明天*下雨*或者*不下雨*这两种结果。但是通常天气预报会给我们一个降雨概率，比如说明天降雨概率80%等等。这时目标函数的结果就不是离散的数值，而是连续的数值了。这时目标函数f(**x**)可以如下表示：

> f(**x**)=P(+1|**x**)∈[0, 1]

其中**x**表示特征向量。

这样所有的输出都是概率的形式了。但是问题是，我们的训练集给我们的信息却是离散的，我们只知道过去的某一天的气象数据是什么（训练集元素的特征向量）和那天下没下雨（训练集元素的标签）。因此，如何使用这些离散的训练集标签去设计出我们想要的可以得出连续结果的函数成为一个重要的问题。

在分类问题和线性回归问题中，我们通常引入**积分（score）**的概念，通过权重向量与特征向量的内积求出每个输入对应的积分：

> ![](http://on5we0xu0.bkt.clouddn.com/17-9-3/62711612.jpg)

很明显，得分的值域是[-∞, +∞]。如何将得分转化成上面的概率值呢？这就是**逻辑斯蒂函数（Logistic Function）**的作用。逻辑斯蒂函数的定义域是[-∞, +∞]，而值域恰好是[0, 1]，正好满足了将积分转化成概率的需求。逻辑斯蒂函数通常用θ(s)表示，大致图像如题图所示，数学表达式如下所示：

> ![](http://on5we0xu0.bkt.clouddn.com/17-9-3/38203967.jpg)

对于这个逻辑斯蒂函数我们可以带几个特殊值验证一下正确性。如θ(-∞)=0，θ(0)=0.5，θ(+∞)=1。可以看到逻辑斯蒂函数可以准确地将实数集映射到(0, 1)区间。

观察函数图象可以看出逻辑斯蒂函数是一个平滑的，单调的S形函数，因此也称Sigmoid函数。

这样我们就可以基于逻辑斯蒂函数定义一个新的函数h(x):

> ![](http://on5we0xu0.bkt.clouddn.com/17-9-3/58944498.jpg)

---

## 2.逻辑斯蒂回归错误

和线性回归的平方错误一样，非线性回归也有自己的错误评价方式。但是由于非线性回归的目标函数给出的是概率，因此其错误也是用概率表示的。

根据上文的目标函数我们可以知道：

> ![](http://images.cnitblog.com/blog/489652/201503/111636169024920.png)

同时，假如我们有数据集D={(**x<sub>1</sub>**, +1), (**x<sub>2</sub>**, -1), ..., (**x<sub>n</sub>**, -1)}，那么根据条件概率可知得到该数据集的概率为：

> P(D)=P(**x<sub>1</sub>**)P(+1|**x<sub>1</sub>**)×P(**x<sub>2</sub>**)P(-1|**x<sub>2</sub>**)×...×P(**x<sub>n</sub>**)P(-1|**x<sub>n</sub>**)

将以上两个式子结合起来就是：

> P(D)=P(**x<sub>1</sub>**)f(**x<sub>1</sub>**)×P(**x<sub>2</sub>**)(1-f(**x<sub>2</sub>**))×...×P(**x<sub>n</sub>**)(1-f(**x<sub>n</sub>**))

但是实际上这个f(x)是未知的，第一部分只给出了它的假设函数h(x)。如果这个h(x)比较接近f(x)的话，我们用h(x)来替代f(x)误差就越小。这个误差就是非线性回归的的**错误（error）**。

我们把用f(x)算出来的P(D)叫做概率（probability），把h(x)替换以后算出来的P(D)叫做似然（likelihood）。替换以后的公式如下所示：

> P(D)=P(**x<sub>1</sub>**)h(**x<sub>1</sub>**)×P(**x<sub>2</sub>**)(1-h(**x<sub>2</sub>**))×...×P(**x<sub>n</sub>**)(1-h(**x<sub>n</sub>**))

根据逻辑斯蒂函数的性质，我们还可以将该式转化成如下所示：

> P(D)=P(**x<sub>1</sub>**)h(**x<sub>1</sub>**)×P(**x<sub>2</sub>**)(h(-**x<sub>2</sub>**))×...×P(**x<sub>n</sub>**)(h(-**x<sub>n</sub>**))

对于每一个已经得到的数据集D而言，我们希望它对应的似然越大越好，因为似然越大，说明我们的h(x)越接近真实的f(x)，说明我们的假设越准确。因此我们所要最好的假设g(x)可以表示如下：

> ![](http://images.cnitblog.com/blog/489652/201503/111636224495881.png)

可以观察到P(**x<sub>n</sub>**)的大小是固定的，不影响似然的结果。因此我们可以得出结论：似然的大小只与h(x)对每个样本的连乘有关：

> ![](http://images.cnitblog.com/blog/489652/201503/111636279648598.png)

其中y<sub>n</sub>是样本n的标签。接下来我们就通过这个式子寻找最大似然：

> ![](http://images.cnitblog.com/blog/489652/201503/111636298863387.png)

连乘式寻找极值的过程太过复杂，因此我们可以考虑用对数将连乘转化成连加：

> ![](http://images.cnitblog.com/blog/489652/201503/111636307778058.png)

另外这个式子是一个求最大值的式子。在线性回归中，我们用最小平方误差的方式去衡量错误，因此这里我们也最好用最小的平均值形式去表示错误。我们可以在式子前加一个`-1/N`来解决这个问题：

> ![](http://images.cnitblog.com/blog/489652/201503/111636313705173.png)

把逻辑斯蒂函数的表达式代入上式：

> ![](http://images.cnitblog.com/blog/489652/201503/111636325422631.png)

我们令err(**w**,**x**,y)=ln(1+exp(-y**w<sup>T</sup>x**))，可得：

> ![](http://images.cnitblog.com/blog/489652/201503/111636346998490.png)

err(**w**,**x**,y)称作交叉熵错误（cross-entropy error）。

---

## 3.逻辑斯蒂回归错误的梯度

既然我们已经推导出错误函数err(**w**,**x**,y)，那么下一步就是将这个错误函数极小化，即找出令该函数值最小的**w**。

错误函数的表达式如下：

> ![](http://images.cnitblog.com/blog/489652/201503/111636401521423.png)

观察这个函数可以看出，这个函数是一个处处连续可微的凸函数（二阶方向导数均恒为正），因此可以知道当梯度为0时函数取最小值，即▽E<sub>in</sub>(**w**)=0。通过计算错误函数对**w**求一阶梯度可以得到：

> ![](http://on5we0xu0.bkt.clouddn.com/17-9-7/58408988.jpg)

根据逻辑斯蒂函数的表达式，我们还可以将该式改写成：

> ![](http://images.cnitblog.com/blog/489652/201503/111636463701756.png)

接下来我们的目的就是让这个式子等于0。观察▽E<sub>in</sub>(**w**)可以看出这是一个以θ函数作为权值，关于(-y<sub>n</sub>**x<sub>n</sub>**)的加权求和函数。首先假设一种极端情况，即权值均为0的情况，即θ(-y<sub>n</sub>**w<sup>T</sup>x**)都等于0，这样可以得出-y<sub>n</sub>**w<sup>T</sup>x**趋近于负无穷，即y<sub>n</sub>**w<sup>T</sup>x**	≫0。这就意味着所有的y<sub>n</sub>都和**w<sup>T</sup>x**同号，即线性可分。

除了这种特殊情况，单纯求这个加权求和函数为零的解是没有闭式解的解法。那么该怎么办呢？

既然无法一步到位找出闭式解，我们可以考虑迭代的方式。这就是下面提到的**梯度下降（Gradient Descent）**思想。

---

## 4.梯度下降

梯度下降是一种迭代优化算法，其思路类似于PLA，通过一步一步地更改权值向量**w**，最后得到最小的E<sub>in</sub>(**w**)。迭代的更新公式如下：

> ![](http://images.cnitblog.com/blog/489652/201503/111637040746297.png)

在梯度下降问题中，公式中的更新步长η（恒大于0）和更新方向**v**（**v**是单位向量，只表示方向）是更新的关键。

上文提到，E<sub>in</sub>(**w**)是一个处处连续可微的凸函数，图像大致如下：

![](http://images.cnitblog.com/blog/489652/201503/111637100584560.png)

我们为了快速求出最小的E<sub>in</sub>(**w**)，就需要快速找到让E<sub>in</sub>(**w**)值达到图像“谷底”的**w**。假如我们的步长η已知，那么我们的方向**v**只有是朝向“谷底”最陡峭的方向才是最快的更新方法。即我们要找出下式作为我们下一轮迭代时的E<sub>in</sub>(**w**)：

> ![](http://images.cnitblog.com/blog/489652/201503/111637179807679.png)

但是这个式子中的η**v**是在自变量中的，仍然不是一个线性优化问题。因此我们可以用到泰勒展开公式，将极小一段曲线看作一个线段寻找近似解：

> ![](http://images.cnitblog.com/blog/489652/201503/111637187147836.png)

因此，在η充分小的情况下，E<sub>in</sub>(**w**)的一阶泰勒展开如下：

> ![](http://images.cnitblog.com/blog/489652/201503/111637202611811.png)

这样求最小值的方法就简单多了。由于E<sub>in</sub>(**w<sub>t</sub>**)和η是已知的，因此最小化的问题就转化成如下形式：
> ![](http://images.cnitblog.com/blog/489652/201503/111637284803488.png)

两个向量点乘时，方向相反时乘积最小，又因为**v**是单位向量，所以**v**可以如下表示：

> ![](http://images.cnitblog.com/blog/489652/201503/111637306369346.png)

因此，在η充分小的情况下，新的更新公式如下：

> ![](http://images.cnitblog.com/blog/489652/201503/111637323397432.png)

可以观察到，这个公式中每一次迭代**w**都会像梯度的反方向移动一点，直至到达“谷底”，使E<sub>in</sub>(**w**)最小。因此这种方法得名梯度下降。

在刚刚的推导中，我们假设η已知。但是实际上η的选择也是十分重要的，它对梯度下降的结果与效率都有着相当大的影响。

![](http://images.cnitblog.com/blog/489652/201503/111637345893320.png)

当η太小时，**w**的下降速度太慢，效率太低；而当η太大时，每次**w**的变化太大，下降不稳定，很有可能得不到准确的结果。一个恰当的η应该随着梯度的变化而变化，当梯度大的时候下降得快一些，反之亦然。因此η应该是与梯度成正比的。根据这个条件，我们可以定义一个新的固定的η：

> ![](http://images.cnitblog.com/blog/489652/201503/111637422147882.png)

把新的η代入更新公式：

> ![](http://images.cnitblog.com/blog/489652/201503/111637428246469.png)

此时的η叫做**学习速率（learning rate）**，有时也用α表示。

综上，逻辑斯蒂回归算法的流程如下：

1. 设置初始权值向量**w<sub>0</sub>**以及迭代次数变量t；
2. 计算▽E<sub>in</sub>(**w<sub>t</sub>**)；
3. 根据计算出来的梯度值用更新公式更新**w<sub>t</sub>**；
4. 直到▽E<sub>in</sub>(**w<sub>t</sub>**)≈0，或者迭代次数足够多，停止迭代，得到目标的权值向量。

逻辑斯蒂回归算法的Python实现：
```Python
import numpy as np
import random
from numpy import gradient

def logistic(x):
    return 1/(1 + np.exp(-x))

def errin(x):
    return np.log(1 + np.exp(x))

def gradientDescent(x, y, theta, alpha, m, numIterations):
    for i in range(0, numIterations):
        err = 0
        for j in range(0, m):
            err += errin(-y[j] * np.dot(theta, x[j]))
        print("Iteration " + str(i) + "|" + "Err " + str(err))
        gradient = 0
        for j in range(0, m):
            gradient += logistic(-y[j] * np.dot(theta, x[j])) * (-y[j] * x[j])
        gradient /= m
        theta = theta - alpha * gradient
        print(theta)
    return theta
```

---

#### Reference Source:

1. [台湾大学林轩田教授《机器学习基石》](http://www.bilibili.com/video/av4294020/ "http://www.bilibili.com/video/av4294020/")
2. [麦子学院-深度学习基础介绍-机器学习](http://www.maiziedu.com/course/373/ "http://www.maiziedu.com/course/373/")
3. [机器学习基石笔记10——机器可以怎样学习（2）](http://www.cnblogs.com/ymingjingr/p/4330304.html "http://www.cnblogs.com/ymingjingr/p/4330304.html")
